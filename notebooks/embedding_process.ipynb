{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Baixar e Instalar pacotes de bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain_huggingface\n",
    "!pip install langchain_community\n",
    "!pip install pypdf\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sugerir carregar Google Drive para os documentos a serem carregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Carregar documentos e adicionar em uma lista de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Lista para armazenar os documentos carregados\n",
    "diretorio_base='/content/drive/MyDrive/topicos_ia/data/minuta'\n",
    "docs_list = []\n",
    "i = 1\n",
    "\n",
    "# Itera sobre os arquivos no diretório e carrega os PDFs\n",
    "for nome_arquivo in os.listdir(diretorio_base):\n",
    "    if nome_arquivo.endswith(\".pdf\"):  # Verifica se o arquivo é um PDF\n",
    "        caminho_pdf = os.path.join(diretorio_base, nome_arquivo)\n",
    "\n",
    "        loader = PyPDFLoader(caminho_pdf)\n",
    "\n",
    "        print(f'loader: {loader}\\n\\n')\n",
    "                \n",
    "        docs = await loader.aload()\n",
    "\n",
    "        print(f'docs_loader: {docs[0].page_content[:100]}\\n\\n')\n",
    "        print(docs[0].metadata)\n",
    "        \n",
    "        docs_list.append(docs)\n",
    "\n",
    "        print(i)\n",
    "        i += 1\n",
    "\n",
    "    if i == 3:\n",
    "      break\n",
    "\n",
    "\n",
    "print(docs_list)\n",
    "print(len(docs_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir os Documentos em partes menores e adicionar elas em uma lista especifica de cada tamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1ª variação\n",
    "text_splitter_1 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300\n",
    ")\n",
    "\n",
    "# 2ª variação\n",
    "text_splitter_2 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=600\n",
    ")\n",
    "\n",
    "# 3ª variação\n",
    "text_splitter_3 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=900\n",
    ")\n",
    "\n",
    "chunks1_list = []\n",
    "chunks2_list = []\n",
    "chunks3_list = []\n",
    "\n",
    "for i, doc in enumerate(docs_list):\n",
    "  # print(doc[i])\n",
    "  print(doc[i].metadata)\n",
    "  chunk_1 = text_splitter_1.split_text(doc[i].page_content)\n",
    "  chunk_2 = text_splitter_2.split_text(doc[i].page_content)\n",
    "  chunk_3 = text_splitter_3.split_text(doc[i].page_content)\n",
    "\n",
    "  chunks1_list.append(chunk_1)\n",
    "  chunks2_list.append(chunk_2)\n",
    "  chunks3_list.append(chunk_3)\n",
    "\n",
    "\n",
    "print('chunks 1')\n",
    "for i in chunks1_list:\n",
    "  print(i, end='\\n')\n",
    "\n",
    "print('\\nchunks 2')\n",
    "for i in chunks2_list:\n",
    "  print(i, end='\\n')\n",
    "\n",
    "print('\\nchunks 3')\n",
    "for i in chunks3_list:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicionario de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    'all-MiniLM-L6-v2':\n",
    "        'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        \n",
    "    'all-mpnet-base-v2':\n",
    "        'sentence-transformers/all-mpnet-base-v2',\n",
    "        \n",
    "    'multi-qa-MiniLM-L6-cos-v1':\n",
    "        'sentence-transformers/multi-qa-MiniLM-L6-cos-v1',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciar os Embeddings e adicionar em uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_list = []\n",
    "\n",
    "embedding1 = HuggingFaceEmbeddings(model_name=embedding_models['all-MiniLM-L6-v2'])\n",
    "embedding2 = HuggingFaceEmbeddings(model_name=embedding_models['all-mpnet-base-v2'])\n",
    "embedding3 = HuggingFaceEmbeddings(model_name=embedding_models['multi-qa-MiniLM-L6-cos-v1'])\n",
    "\n",
    "embedding_list.append(embedding1)\n",
    "embedding_list.append(embedding2)\n",
    "embedding_list.append(embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixar e instalar os pacotes para o FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install faiss-gpu\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converter novamente os documentos divididos (strings) para objetos Document e criar índices e Embeddings deles, por fim adicionar em uma lista com os embeddings de tamanho especificos que foram criados com base na divisão dos chunks/overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Certificar que cada chunk seja uma string (caso seja uma lista de palavras ou sentenças)\n",
    "chunks1_docs = [Document(page_content=\" \".join(chunk) if isinstance(chunk, list) else chunk) for chunk in chunks1_list]\n",
    "chunks2_docs = [Document(page_content=\" \".join(chunk) if isinstance(chunk, list) else chunk) for chunk in chunks2_list]\n",
    "chunks3_docs = [Document(page_content=\" \".join(chunk) if isinstance(chunk, list) else chunk) for chunk in chunks3_list]\n",
    "\n",
    "\n",
    "# Criar os índices FAISS\n",
    "faiss1_list = []\n",
    "faiss2_list = []\n",
    "faiss3_list = []\n",
    "\n",
    "for i, embedding in enumerate(embedding_list):\n",
    "    print(f\"Gerando índice FAISS com o modelo: {embedding.model_name}\")\n",
    "\n",
    "    faiss_index1 = FAISS.from_documents(chunks1_docs, embedding)\n",
    "    faiss_index2 = FAISS.from_documents(chunks2_docs, embedding)\n",
    "    faiss_index3 = FAISS.from_documents(chunks3_docs, embedding)\n",
    "\n",
    "    # Adicionando os índices a uma lista\n",
    "    faiss1_list.append(faiss_index1)\n",
    "    faiss2_list.append(faiss_index2)\n",
    "    faiss3_list.append(faiss_index3)\n",
    "\n",
    "    # Salvar o índice FAISS localmente\n",
    "    faiss_index1.save_local(\"faiss_index1.index\")\n",
    "    faiss_index2.save_local(\"faiss_index2.index\")\n",
    "    faiss_index3.save_local(\"faiss_index3.index\")\n",
    "\n",
    "    print(\"Índices salvos com sucesso!\")\n",
    "\n",
    "# Carregar os índices FAISS a partir dos arquivos salvos\n",
    "faiss_index1_loaded = FAISS.load_local(\"faiss_index1.index\", embedding1, allow_dangerous_deserialization=True)\n",
    "faiss_index2_loaded = FAISS.load_local(\"faiss_index2.index\", embedding2, allow_dangerous_deserialization=True)\n",
    "faiss_index3_loaded = FAISS.load_local(\"faiss_index3.index\", embedding3, allow_dangerous_deserialization=True)\n",
    "\n",
    "print(\"Índices carregados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testar se a similaridade dos vetores de embeddings são carregadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Acessando os vetores de embeddings a partir do índice FAISS\n",
    "faiss_vectors1 = faiss_index1_loaded.index.reconstruct_n(0, faiss_index1_loaded.index.ntotal)\n",
    "faiss_vectors2 = faiss_index2_loaded.index.reconstruct_n(0, faiss_index2_loaded.index.ntotal)\n",
    "faiss_vectors3 = faiss_index3_loaded.index.reconstruct_n(0, faiss_index3_loaded.index.ntotal)\n",
    "\n",
    "# Visualizando os vetores\n",
    "print(\"Vetores do índice 1:\")\n",
    "print(faiss_vectors1)\n",
    "\n",
    "print(\"Vetores do índice 2:\")\n",
    "print(faiss_vectors2)\n",
    "\n",
    "print(\"Vetores do índice 3:\")\n",
    "print(faiss_vectors3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
