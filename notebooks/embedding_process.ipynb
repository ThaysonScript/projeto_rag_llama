{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain_huggingface\n",
    "!pip install langchain_community\n",
    "!pip install pypdf\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Lista para armazenar os documentos carregados\n",
    "diretorio_base='/content/drive/MyDrive/topicos_ia/data/minuta'\n",
    "docs_list = []\n",
    "i = 1\n",
    "\n",
    "# Itera sobre os arquivos no diretório e carrega os PDFs\n",
    "for nome_arquivo in os.listdir(diretorio_base):\n",
    "    if nome_arquivo.endswith(\".pdf\"):  # Verifica se o arquivo é um PDF\n",
    "        caminho_pdf = os.path.join(diretorio_base, nome_arquivo)\n",
    "\n",
    "        loader = PyPDFLoader(caminho_pdf)\n",
    "\n",
    "        print(f'loader: {loader}\\n\\n')\n",
    "                \n",
    "        docs = await loader.aload()\n",
    "\n",
    "        print(f'docs_loader: {docs[0].page_content[:100]}\\n\\n')\n",
    "        print(docs[0].metadata)\n",
    "        \n",
    "        docs_list.append(docs)\n",
    "\n",
    "        print(i)\n",
    "        i += 1\n",
    "\n",
    "    if i == 3:\n",
    "      break\n",
    "\n",
    "\n",
    "print(docs_list)\n",
    "print(len(docs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1ª variação\n",
    "text_splitter_1 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300\n",
    ")\n",
    "\n",
    "# 2ª variação\n",
    "text_splitter_2 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1800,\n",
    "    chunk_overlap=350\n",
    ")\n",
    "\n",
    "# 3ª variação\n",
    "text_splitter_3 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=400\n",
    ")\n",
    "\n",
    "chunks_list = []\n",
    "\n",
    "for i, doc in enumerate(docs_list):\n",
    "  text = doc[i].page_content\n",
    "  chunk_1 = text_splitter_1.split_text(text)\n",
    "  chunk_2 = text_splitter_2.split_text(text)\n",
    "  chunk_3 = text_splitter_3.split_text(text)\n",
    "\n",
    "  chunks_list.append(chunk_1)\n",
    "  chunks_list.append(chunk_2)\n",
    "  chunks_list.append(chunk_3)\n",
    "\n",
    "\n",
    "\n",
    "print(chunks_list)\n",
    "print(len(chunks_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embedding_models = {\n",
    "    'all-MiniLM-L6-v2':\n",
    "        'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        \n",
    "    'all-mpnet-base-v2':\n",
    "        'sentence-transformers/all-mpnet-base-v2',\n",
    "        \n",
    "    'multi-qa-MiniLM-L6-cos-v1':\n",
    "        'sentence-transformers/multi-qa-MiniLM-L6-cos-v1',\n",
    "        \n",
    "    'all-distilroberta-v1':\n",
    "        'sentence-transformers/all-distilroberta-v1',\n",
    "        \n",
    "    'gte-small':\n",
    "        'thenlper/gte-small',\n",
    "        \n",
    "    'pubmedbert-base-embeddings':\n",
    "        'NeuML/pubmedbert-base-embeddings' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_list = []\n",
    "\n",
    "embedding1 = HuggingFaceEmbeddings(model_name=embedding_models['all-MiniLM-L6-v2'])\n",
    "embedding2 = HuggingFaceEmbeddings(model_name=embedding_models['all-mpnet-base-v2'])\n",
    "embedding3 = HuggingFaceEmbeddings(model_name=embedding_models['multi-qa-MiniLM-L6-cos-v1'])\n",
    "embedding4 = HuggingFaceEmbeddings(model_name=embedding_models['all-distilroberta-v1'])\n",
    "embedding5 = HuggingFaceEmbeddings(model_name=embedding_models['gte-small'])\n",
    "embedding6 = HuggingFaceEmbeddings(model_name=embedding_models['pubmedbert-base-embeddings'])\n",
    "\n",
    "embedding_list.append(embedding1)\n",
    "embedding_list.append(embedding2)\n",
    "embedding_list.append(embedding3)\n",
    "embedding_list.append(embedding4)\n",
    "embedding_list.append(embedding5)\n",
    "embedding_list.append(embedding6)\n",
    "\n",
    "print(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS(embedding_function=embeddings)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
